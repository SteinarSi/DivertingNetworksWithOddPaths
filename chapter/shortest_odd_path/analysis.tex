\section{Analysis}
% The algorithm does not solve \textsc{Shortest Odd Path} in the absolute general case but rather has some limitations. These are: 
% \begin{itemize}
%     \item The input graph must be undirected, otherwise we cannot use blossoms the way we do.
%     \item The edges must have either non-negative weights or no weights at all, otherwise we cannot guarantee that $d^+_u$ and $d^-_u$ are correct when we scan a vertex $u$.
% \end{itemize}
% \todo{Should this be written somewhere else?}

\begin{theorem}
    Let $(G,s,t)$ be an instance of \textsc{Shortest Odd Path}, let $n~:= |V|$ and let $m~:= |E|$. \\
    Claim: the algorithm runs in time at most $O(m \cdot \log m)$, or $O(m \cdot \log n)$ of the graph is simple.
    \begin{proof}  
        First of all, we construct the mirror graph $M$ with $2n-2 \in O(n)$ vertices and $2m - \deg(s) - \deg(t) \in O(m)$ edges, in time $O(n+m)$.
        
        With our \pyth{completed} array we can guarantee that each vertex is scanned at most once, and the scanning operation just loops through all the neighbors. Therefore, the total cost of all the scans is $O(n + \sum_{u \in V} \deg(u)) = O(n + 2m) = O(n + m)$.
    
        The blossom operation is a little more convoluted. Thanks to the overly complicated code in our \pyth{backtrack_blossom} procedure in \Cref{subsection:backtracking-blossom-edges}, we can backtrack from a blossom edge and determine the vertices in the blossom in time linear to the size of the blossom. Setting their values for $d^+$ and $d^-$ can also be done in linear time, and the potential scans have already been accounted for above. The key point here is that we shrink the blossom into a pseudonode afterwards: each vertex can then only be part of such a blossom procedure at most once. Even though we may compute many blossom edges, the total amount of work will therefore never exceed $O(n)$.
    
        Finally, we have the main loop, which iteratively pops vertices and blossom edges from the queue. Each of the $O(n)$ vertices may be put into the priority queue many times, at most once for each of its neighbors. That is a total of $O(m)$ vertices in the queue, for a total cost of $O(m)$. Though it is unlikely, in the extreme case all edges may be put in the queue as blossom edges as well, again for a total cost of $O(m)$. In total, enqueueing everything costs $O(m)$, and dequeueing everything costs $O(m \cdot \log m)$.
    
        In total, the algorithm runs in time $O(n+m) + O(n+m) + O(n) + O(m) + O(m \cdot \log m) = O(m \cdot \log m)$, which shows the first part of the claim. If the graph is simple, then $O(m \cdot \log m) \subseteq O(m \cdot \log n^2) = O(m \cdot 2 \cdot \log n) = O(m \cdot \log n)$, which shows the second part of the claim.
    \end{proof}        
\end{theorem}

\subsection{Other variants}
A running time of $O(m \cdot \log n)$ means that the algorithm generally performs well on sparse graphs. We chose this algorithm with this running time because in \Cref{chapter:network-diversion} we will use it on planar graphs, where $m \leq 3n-6$, as shown in \Cref{corollary:m-leq-3n}. That means it runs in $O(n \log n)$ on planar graphs.

We should note, however, that there are also other known polynomial algorithms for \textsc{Odd Shortest Path}. In the same paper that Derigs gave the algorithm of this chapter, he also gave another variant \cite{source:derigs_shortest_odd_path}. The main difference is that we drop the priority queues and use a list instead, and in the control loop instead search through the entire list and pop the element with the lowest priority. That search takes time at most $O(n)$, and can be done at most $n$ times, for a total running time of $O(n^2)$. If the input graphs are dense, then this $O(n^2)$ algorithm may be preferable to our $O(m \log n)$ algorithm.

\todo[inline]{Nevn de andre variantene, og hvor fort de kan l√∏ses. $O(n+m)$ for uvektede grafer er ganske nice.}

\subsection{Benchmarking different data structures for the Basis}
\label{subsubsection:testing-basis}
As explained in \Cref{subsection:basis-code}, we have developed multiple data structures to keep track of the basis of each vertex. One of them is based on the Observer pattern, the other on the well-known UnionFind structure. We have tested both on seven sparse graphs from the real world, as well as on four synthetic Delaunay graphs, and show the results below. We ran our shortest odd path algorithm on each graph 40 times, 20 for each structure, and noted the average times for each. See \Cref{section:benchmarking} for more specific information about how these benchmarks are done.
\begin{center}
    \begin{tabular}{|l | r | r | r | r | c|} 
        \hline
        Graph & n & m & Observer & UF & Change \\ [0.5ex] 
        \hline\hline
        Power BCSPWR09 \cite{graph:bcspwr09} & 1723 & 2394 & 1.7ms & 1.5ms & -12\% \\
        \hline
        Oldenburg \cite{graph:oldenburg-sf-cal-san-roads-etc} & 6106 & 7035 & 5.6ms & 4.8ms & -14\%\\ 
        \hline
        San Joaquin County \cite{graph:oldenburg-sf-cal-san-roads-etc} & 18263 & 23874 & 22ms & 18ms & -18\%\\
        \hline
        Cali Road Network \cite{graph:oldenburg-sf-cal-san-roads-etc} & 21048 & 21693 & 21ms & 18ms & -14\%\\
        \hline
        Musae Github \cite{graph:musae-github} & 37700 & 289003 & 125ms & 123ms & -1.5\%\\
        \hline
        SF Road Network \cite{graph:oldenburg-sf-cal-san-roads-etc} & 174956 & 223001 & 225ms & 191ms & -15\%\\
        \hline
        Ca Citeseer \cite{graph:ca-citeseer} & 227321 & 814137 & 629ms & 608ms & -3\%\\
        \hline
        \hline
        Delaunay 50k & 50000 & 149961 & 106ms & 96ms & -9\%\\
        \hline
        Delaunay 100k & 100000 & 299959 & 219ms & 205ms & -6\%\\
        \hline
        Delaunay 150k & 150000 & 449965 & 346ms & 315ms & -9\%\\
        \hline
        Delaunay 200k & 200000 & 599961 & 476ms & 449ms & -6\%\\
        \hline
    \end{tabular}
\end{center}

As we can see, the UnionFind-based structure either outperforms or does as well as the Observer-based structure on all the graphs we tested. Most notably, we have an 18\% decrease in time spent on the graph of San Joaquin County. There seems to be pattern where it performs better if the graphs are sparser. This makes sense: the UnionFind-based structure updates the base of vertices very fast and spends more time to lookup bases for each potential blossom edge, while the Observer-based structure does the exact opposite. The focus of this thesis is to create an algorithm that performs well on sparse graphs, especially planar graphs, which is why we mostly test on sparse graphs. If we instead had implemented an algorithm for denser graphs, or just graphs in general, then testing on graphs with more edges would be more appropriate. Therefore we will use the UnionFind-based structure as the default in our codebase, as well as in the remainder of this thesis, and note to the reader that the other option might be better in denser graphs.

\subsection{Running times on Delaunay graphs}
\label{subsubsection:odd-path-delaunay-testing}
To test how well the algorithm scales, we generate 200 Delaunay graphs of sizes 1000, 2000, 3000, and so on until 200k. We explain how these graphs are generated in \Cref{section:benchmarking}. For each graph, we have estimated a source and target with the maximum shortest path between them, and run our \textsc{Shortest Odd Path} algorithm. We take the median running time of 10 runs and plot the results below.

The theoretical running time is $O(m \log n)$, and we have tried our best to come up with constants to create a running time function that matches our algorithm. In Delaunay graphs, we have that $m \leq 3n$, so we have set $m~:= 3n$ in this function.

\begin{center}
    \includesvg[width=0.9\textwidth]{figures/bench_plots/shortest odd path.svg}
\end{center}

As we can see, the running times grow almost linearly as the inputs grow larger. The slight upward curve is barely noticeable, and this is to be expected with a linearithmic theoretical running time. We are very happy with these results. Being able to solve \textsc{Shortest Odd Path} on sparse graphs of 100k vertices in a fifth of a second is exactly the kind of performance we were hoping for. Now we can take this algorithm with us and solve our next challenge: \textsc{Network Diversion}.
